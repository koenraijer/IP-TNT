{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size:  80.23809523809524\n",
      "Val size:  8.333333333333332\n",
      "Test size:  11.428571428571429\n",
      "Size: : (1011, 768, 6)\n",
      "Initial imputation complete.\n",
      "Missing values before outlier detection:\n",
      "   Train  Validation  Test\n",
      "0   0.00        0.00  0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/koenraijer/Documents/00_Werk/Data_science/Thesis/Analysis/helpers.py:746: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final imputation complete.\n",
      "  Feature  Train  Validation  Test\n",
      "0       0   0.59        0.00  0.00\n",
      "1       1  15.58       14.62  6.23\n",
      "2       2  10.81        0.00  0.00\n",
      "3       3  11.32       15.51 13.71\n",
      "4       4  13.67        0.00  0.00\n",
      "5       5  23.94        9.54 13.29\n"
     ]
    }
   ],
   "source": [
    "# IMPORTS\n",
    "# import sys\n",
    "# sys.path.insert(0, '../Analysis')\n",
    "import helpers as h\n",
    "import empatica_helpers as eh\n",
    "import inquisit_helpers as ih\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from importlib import reload\n",
    "import pickle\n",
    "\n",
    "# ML IMPORTS\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "reload(h), reload(eh), reload(ih)\n",
    "\n",
    "# GLOBAL SETTINGS\n",
    "pd.set_option('display.max_rows', 200)\n",
    "pd.options.display.float_format = '{:.2f}'.format\n",
    "plt.rcParams[\"figure.figsize\"] = (20, 10)\n",
    "plt.style.use('seaborn-v0_8-notebook') # plt.style.use('ggplot'); print(plt.style.available)\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "sr = 32\n",
    "wl = 24 # Window length in seconds\n",
    "\n",
    "# FULL PIPELINE\n",
    "# e_raw, _ = eh.load_empatica(data_folder='input/empatica/', useIBI=False, save=True, plotTrimmings=False, desired_sampling_rate=sr)\n",
    "# i_raw = ih.load_inquisit(data_folder='input/inquisit/', save=True)\n",
    "# ei_raw = h.combine_empatica_and_inquisit(e_raw, i_raw, save=True, sr=sr)\n",
    "# ei_prep = h.clean_and_filter(save=True, normalise=None, sr=sr, window_length=wl)\n",
    "# X, y, p = h.prepare_for_vae(sr=sr, wl=wl, filepath=\"output/ei_prep_original.csv\", save=True, normalise=None) # Normalisation now happens later in the process. Normalise = False applies the standard scaler to the data.\n",
    "X_train, X_val, X_test, y_train, y_val, y_test, p_train, p_val, p_test = h.prepare_train_val_test_sets(filenames=['output/dl_X_wl24_sr32_original.pkl', 'output/dl_y_wl24_sr32_original.pkl', 'output/dl_p_wl24_sr32_original.pkl'])\n",
    "X_train, X_val, X_test = h.handle_outliers_and_impute(X_train, X_val, X_test, num_mad=4, verbose=True)\n",
    "X_train, X_val, X_test = h.scale_features(X_train, X_val, X_test, p_train, p_val, p_test, normalise=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handle outliers and impute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Robust Mahalanobis distance (computationally way too expensive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.covariance import MinCovDet\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "import numpy as np\n",
    "\n",
    "def handle_outliers_and_impute(X_train, X_val, X_test, random_state=0):\n",
    "    # Number of features\n",
    "    num_features = X_train.shape[2]\n",
    "\n",
    "    for feature in range(num_features):\n",
    "        print(f\"Processing feature {feature+1} out of {num_features}\")\n",
    "        \n",
    "        # Select the feature from each dataset\n",
    "        X_train_feature = X_train[:, :, feature]\n",
    "        X_val_feature = X_val[:, :, feature]\n",
    "        X_test_feature = X_test[:, :, feature]\n",
    "\n",
    "        # Fit the IterativeImputer on the training set and transform training, validation, and test sets\n",
    "        imputer = IterativeImputer(random_state=random_state)\n",
    "        X_train_feature = imputer.fit_transform(X_train_feature)\n",
    "        X_val_feature = imputer.transform(X_val_feature)\n",
    "        X_test_feature = imputer.transform(X_test_feature)\n",
    "        print(f\"Feature {feature}: pre-outlier detection imputation complete.\")\n",
    "        \n",
    "        # Robust Mahalanobis Distance\n",
    "        robust_cov = MinCovDet().fit(X_train_feature)\n",
    "        maha_dist = robust_cov.mahalanobis(X_train_feature)\n",
    "        threshold = np.percentile(maha_dist, 99.7)  # 3 standard deviations\n",
    "        outliers_train = maha_dist > threshold\n",
    "        print(f\"Feature {feature}: Percentage of outliers in training set: {np.mean(outliers_train) * 100:.2f}%\")\n",
    "\n",
    "        # Identify outliers in validation and test sets\n",
    "        maha_dist_val = robust_cov.mahalanobis(X_val_feature)\n",
    "        outliers_val = maha_dist_val > threshold\n",
    "        print(f\"Feature {feature}: Percentage of outliers in validation set: {np.mean(outliers_val) * 100:.2f}%\")\n",
    "\n",
    "        maha_dist_test = robust_cov.mahalanobis(X_test_feature)\n",
    "        outliers_test = maha_dist_test > threshold\n",
    "        print(f\"Feature {feature}: Percentage of outliers in test set: {np.mean(outliers_test) * 100:.2f}%\")\n",
    "\n",
    "        # Create a mask for inliers\n",
    "        inliers_train = ~outliers_train\n",
    "\n",
    "        # Fit the IterativeImputer on the inliers in the training set\n",
    "        imputer = IterativeImputer(random_state=random_state)\n",
    "        imputer.fit(X_train_feature[inliers_train])\n",
    "\n",
    "        # Use the fitted imputer to predict the values for the outliers in the training, validation, and test sets\n",
    "        for dataset, outliers in zip([X_train_feature, X_val_feature, X_test_feature], [outliers_train, outliers_val, outliers_test]):\n",
    "            dataset[outliers] = imputer.transform(dataset[outliers.reshape(-1,1)])\n",
    "\n",
    "        # Assign the processed feature back to the original datasets\n",
    "        X_train[:, :, feature] = X_train_feature\n",
    "        X_val[:, :, feature] = X_val_feature\n",
    "        X_test[:, :, feature] = X_test_feature\n",
    "\n",
    "    return X_train, X_val, X_test\n",
    "\n",
    "X_train, X_val, X_test = handle_outliers_and_impute(X_train, X_val, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial imputation complete.\n",
      "Missing values before outlier detection:\n",
      "   Train  Validation  Test\n",
      "0   0.00        0.00  0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/4y/grqbq6j10592pmzzhgzbxdmr0000gn/T/ipykernel_71325/3843394787.py:57: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  outliers_df = pd.concat([outliers_df, pd.DataFrame({\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final imputation complete.\n",
      "  Feature  Train  Validation  Test\n",
      "0       0   2.28        3.60  0.00\n",
      "1       1  18.39       31.15 15.65\n",
      "2       2   6.89        2.01  0.38\n",
      "3       3  10.19        9.69 16.01\n",
      "4       4  13.12        0.02 14.30\n",
      "5       5  27.09       22.64 28.72\n",
      "Initial imputation complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/4y/grqbq6j10592pmzzhgzbxdmr0000gn/T/ipykernel_71325/3843394787.py:57: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  outliers_df = pd.concat([outliers_df, pd.DataFrame({\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final imputation complete.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "import numpy as np\n",
    "from scipy.stats import median_abs_deviation\n",
    "import pandas as pd\n",
    "\n",
    "def handle_outliers_and_impute(X_train, X_val, X_test, random_state=42, num_mad=3, verbose=False):\n",
    "    # Number of features\n",
    "    num_features = X_train.shape[2]\n",
    "\n",
    "    # Impute missing values before outlier detection\n",
    "    imputer = IterativeImputer(random_state=random_state)\n",
    "\n",
    "    # Reshape the data to 2D, impute, then reshape back to 3D\n",
    "    X_train_shape = X_train.shape\n",
    "    X_val_shape = X_val.shape\n",
    "    X_test_shape = X_test.shape\n",
    "    \n",
    "    X_train = imputer.fit_transform(X_train.reshape(-1, X_train_shape[-1])).reshape(X_train_shape)\n",
    "    X_val = imputer.transform(X_val.reshape(-1, X_val_shape[-1])).reshape(X_val_shape)\n",
    "    X_test = imputer.transform(X_test.reshape(-1, X_test_shape[-1])).reshape(X_test_shape)\n",
    "\n",
    "    print(\"Initial imputation complete.\")\n",
    "\n",
    "    # Print missing values\n",
    "    if verbose:\n",
    "        print(\"Missing values before outlier detection:\")\n",
    "        print(pd.DataFrame({\n",
    "            'Train': [np.mean(np.isnan(X_train))],\n",
    "            'Validation': [np.mean(np.isnan(X_val))],\n",
    "            'Test': [np.mean(np.isnan(X_test))]\n",
    "        }))\n",
    "\n",
    "    # Initialize arrays to store outliers\n",
    "    outliers_train = np.zeros_like(X_train, dtype=bool)\n",
    "    outliers_val = np.zeros_like(X_val, dtype=bool)\n",
    "    outliers_test = np.zeros_like(X_test, dtype=bool)\n",
    "\n",
    "    # Initialize DataFrame to store percentage of outliers\n",
    "    outliers_df = pd.DataFrame(columns=['Feature', 'Train', 'Validation', 'Test'])\n",
    "\n",
    "    for feature in range(num_features):\n",
    "        # Select the feature from each dataset\n",
    "        X_train_feature = X_train[:, :, feature]\n",
    "        X_val_feature = X_val[:, :, feature]\n",
    "        X_test_feature = X_test[:, :, feature]\n",
    "\n",
    "        # Median Absolute Deviation\n",
    "        mad = median_abs_deviation(X_train_feature)\n",
    "        threshold = num_mad * mad  # 3x median absolute deviation as threshold\n",
    "\n",
    "        outliers_train[:, :, feature] = np.abs(X_train_feature - np.median(X_train_feature)) > threshold\n",
    "        outliers_val[:, :, feature] = np.abs(X_val_feature - np.median(X_val_feature)) > threshold\n",
    "        outliers_test[:, :, feature] = np.abs(X_test_feature - np.median(X_test_feature)) > threshold\n",
    "\n",
    "        # Add percentage of outliers to DataFrame\n",
    "        outliers_df = pd.concat([outliers_df, pd.DataFrame({\n",
    "            'Feature': feature,\n",
    "            'Train': np.mean(outliers_train[:, :, feature]) * 100,\n",
    "            'Validation': np.mean(outliers_val[:, :, feature]) * 100,\n",
    "            'Test': np.mean(outliers_test[:, :, feature]) * 100\n",
    "        }, index=[0])], ignore_index=True)\n",
    "\n",
    "    # Replace outliers with np.nan in the original datasets\n",
    "    X_train = np.where(outliers_train, np.nan, X_train)\n",
    "    X_val = np.where(outliers_val, np.nan, X_val)\n",
    "    X_test = np.where(outliers_test, np.nan, X_test)\n",
    "\n",
    "    # Impute missing values after outlier detection\n",
    "    X_train = imputer.fit_transform(X_train.reshape(-1, X_train_shape[-1])).reshape(X_train_shape)\n",
    "    X_val = imputer.transform(X_val.reshape(-1, X_val_shape[-1])).reshape(X_val_shape)\n",
    "    X_test = imputer.transform(X_test.reshape(-1, X_test_shape[-1])).reshape(X_test_shape)\n",
    "\n",
    "    print(\"Final imputation complete.\")\n",
    "\n",
    "    # Print DataFrame of outliers\n",
    "    if verbose:\n",
    "        print(outliers_df)\n",
    "\n",
    "    return X_train, X_val, X_test\n",
    "\n",
    "X_train_imp_3, X_val_imp_3, X_test_imp_3 = handle_outliers_and_impute(X_train, X_val, X_test, verbose=True)\n",
    "X_train_imp_4, X_val_imp_4, X_test_imp_4 = handle_outliers_and_impute(X_train, X_val, X_test, num_mad=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Per-participant outlier detection and imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/koenraijer/Documents/00_Werk/Data_science/Thesis/Analysis/.conda/lib/python3.11/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/Users/koenraijer/Documents/00_Werk/Data_science/Thesis/Analysis/.conda/lib/python3.11/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/var/folders/4y/grqbq6j10592pmzzhgzbxdmr0000gn/T/ipykernel_71325/1353084030.py:42: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  outliers_df = pd.concat([outliers_df, pd.DataFrame({\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imputation complete.\n",
      "   Feature  Train  Validation  Test\n",
      "0        0    NaN         NaN 28.48\n",
      "1        1    NaN         NaN 10.56\n",
      "2        2    NaN         NaN  8.81\n",
      "3        3    NaN         NaN 13.63\n",
      "4        4    NaN         NaN  2.30\n",
      "5        5    NaN         NaN 23.90\n",
      "6        0   3.65         NaN   NaN\n",
      "7        1  22.73         NaN   NaN\n",
      "8        2  10.09         NaN   NaN\n",
      "9        3  19.14         NaN   NaN\n",
      "10       4  17.63         NaN   NaN\n",
      "11       5  26.02         NaN   NaN\n",
      "12       0   3.68         NaN   NaN\n",
      "13       1  14.37         NaN   NaN\n",
      "14       2  12.07         NaN   NaN\n",
      "15       3  20.10         NaN   NaN\n",
      "16       4   4.58         NaN   NaN\n",
      "17       5  29.00         NaN   NaN\n",
      "18       0  15.42         NaN   NaN\n",
      "19       1  29.09         NaN   NaN\n",
      "20       2  11.74         NaN   NaN\n",
      "21       3  11.87         NaN   NaN\n",
      "22       4  22.22         NaN   NaN\n",
      "23       5  27.48         NaN   NaN\n",
      "24       0  36.83         NaN   NaN\n",
      "25       1  15.74         NaN   NaN\n",
      "26       2   1.04         NaN   NaN\n",
      "27       3  10.62         NaN   NaN\n",
      "28       4   2.65         NaN   NaN\n",
      "29       5  28.55         NaN   NaN\n",
      "30       0   9.88         NaN   NaN\n",
      "31       1  13.94         NaN   NaN\n",
      "32       2  21.43         NaN   NaN\n",
      "33       3  24.15         NaN   NaN\n",
      "34       4   4.10         NaN   NaN\n",
      "35       5  19.81         NaN   NaN\n",
      "36       0    NaN       24.96   NaN\n",
      "37       1    NaN       11.99   NaN\n",
      "38       2    NaN       11.08   NaN\n",
      "39       3    NaN       19.05   NaN\n",
      "40       4    NaN        0.03   NaN\n",
      "41       5    NaN       21.83   NaN\n",
      "42       0   6.65         NaN   NaN\n",
      "43       1  22.80         NaN   NaN\n",
      "44       2  24.30         NaN   NaN\n",
      "45       3   9.08         NaN   NaN\n",
      "46       4   3.66         NaN   NaN\n",
      "47       5  26.19         NaN   NaN\n",
      "48       0   0.00         NaN   NaN\n",
      "49       1  12.05         NaN   NaN\n",
      "50       2  15.16         NaN   NaN\n",
      "51       3  15.57         NaN   NaN\n",
      "52       4  13.24         NaN   NaN\n",
      "53       5  19.50         NaN   NaN\n",
      "54       0    NaN         NaN 21.41\n",
      "55       1    NaN         NaN 15.96\n",
      "56       2    NaN         NaN 20.55\n",
      "57       3    NaN         NaN 16.74\n",
      "58       4    NaN         NaN 13.75\n",
      "59       5    NaN         NaN 23.97\n",
      "60       0  27.84         NaN   NaN\n",
      "61       1  10.97         NaN   NaN\n",
      "62       2  12.89         NaN   NaN\n",
      "63       3  25.32         NaN   NaN\n",
      "64       4  10.39         NaN   NaN\n",
      "65       5  26.34         NaN   NaN\n",
      "66       0    NaN         NaN  0.00\n",
      "67       1    NaN         NaN  9.97\n",
      "68       2    NaN         NaN  6.20\n",
      "69       3    NaN         NaN 11.18\n",
      "70       4    NaN         NaN 15.68\n",
      "71       5    NaN         NaN 20.13\n",
      "72       0   0.66         NaN   NaN\n",
      "73       1  20.96         NaN   NaN\n",
      "74       2  12.98         NaN   NaN\n",
      "75       3  22.66         NaN   NaN\n",
      "76       4  12.79         NaN   NaN\n",
      "77       5  25.05         NaN   NaN\n",
      "78       0  27.27         NaN   NaN\n",
      "79       1  11.54         NaN   NaN\n",
      "80       2  17.47         NaN   NaN\n",
      "81       3  21.68         NaN   NaN\n",
      "82       4  31.57         NaN   NaN\n",
      "83       5  25.62         NaN   NaN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/koenraijer/Documents/00_Werk/Data_science/Thesis/Analysis/.conda/lib/python3.11/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/Users/koenraijer/Documents/00_Werk/Data_science/Thesis/Analysis/.conda/lib/python3.11/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/var/folders/4y/grqbq6j10592pmzzhgzbxdmr0000gn/T/ipykernel_71325/1353084030.py:42: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  outliers_df = pd.concat([outliers_df, pd.DataFrame({\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imputation complete.\n",
      "   Feature  Train  Validation  Test\n",
      "0        0    NaN         NaN 28.47\n",
      "1        1    NaN         NaN  6.00\n",
      "2        2    NaN         NaN  3.31\n",
      "3        3    NaN         NaN  6.32\n",
      "4        4    NaN         NaN  1.82\n",
      "5        5    NaN         NaN 19.24\n",
      "6        0   0.00         NaN   NaN\n",
      "7        1  17.82         NaN   NaN\n",
      "8        2   6.86         NaN   NaN\n",
      "9        3  14.17         NaN   NaN\n",
      "10       4  15.29         NaN   NaN\n",
      "11       5  21.63         NaN   NaN\n",
      "12       0   0.00         NaN   NaN\n",
      "13       1   9.80         NaN   NaN\n",
      "14       2   3.71         NaN   NaN\n",
      "15       3  13.66         NaN   NaN\n",
      "16       4   0.89         NaN   NaN\n",
      "17       5  24.65         NaN   NaN\n",
      "18       0  15.24         NaN   NaN\n",
      "19       1  25.91         NaN   NaN\n",
      "20       2  11.11         NaN   NaN\n",
      "21       3   9.02         NaN   NaN\n",
      "22       4  18.95         NaN   NaN\n",
      "23       5  23.35         NaN   NaN\n",
      "24       0  36.02         NaN   NaN\n",
      "25       1  10.45         NaN   NaN\n",
      "26       2   0.00         NaN   NaN\n",
      "27       3   7.04         NaN   NaN\n",
      "28       4   1.18         NaN   NaN\n",
      "29       5  24.27         NaN   NaN\n",
      "30       0   6.42         NaN   NaN\n",
      "31       1   7.55         NaN   NaN\n",
      "32       2   2.67         NaN   NaN\n",
      "33       3  19.00         NaN   NaN\n",
      "34       4   1.32         NaN   NaN\n",
      "35       5  15.83         NaN   NaN\n",
      "36       0    NaN       18.24   NaN\n",
      "37       1    NaN        7.13   NaN\n",
      "38       2    NaN        4.14   NaN\n",
      "39       3    NaN       14.46   NaN\n",
      "40       4    NaN        0.00   NaN\n",
      "41       5    NaN       17.81   NaN\n",
      "42       0   0.86         NaN   NaN\n",
      "43       1  16.89         NaN   NaN\n",
      "44       2  17.37         NaN   NaN\n",
      "45       3   6.34         NaN   NaN\n",
      "46       4   0.00         NaN   NaN\n",
      "47       5  22.92         NaN   NaN\n",
      "48       0   0.00         NaN   NaN\n",
      "49       1   6.41         NaN   NaN\n",
      "50       2   8.77         NaN   NaN\n",
      "51       3  12.23         NaN   NaN\n",
      "52       4   1.51         NaN   NaN\n",
      "53       5  13.85         NaN   NaN\n",
      "54       0    NaN         NaN 12.50\n",
      "55       1    NaN         NaN  9.70\n",
      "56       2    NaN         NaN 20.00\n",
      "57       3    NaN         NaN 11.04\n",
      "58       4    NaN         NaN 10.49\n",
      "59       5    NaN         NaN 19.88\n",
      "60       0  16.79         NaN   NaN\n",
      "61       1   5.32         NaN   NaN\n",
      "62       2   6.55         NaN   NaN\n",
      "63       3  21.39         NaN   NaN\n",
      "64       4   4.44         NaN   NaN\n",
      "65       5  21.72         NaN   NaN\n",
      "66       0    NaN         NaN  0.00\n",
      "67       1    NaN         NaN  4.78\n",
      "68       2    NaN         NaN  1.86\n",
      "69       3    NaN         NaN  8.18\n",
      "70       4    NaN         NaN  0.01\n",
      "71       5    NaN         NaN 13.97\n",
      "72       0   0.00         NaN   NaN\n",
      "73       1  15.45         NaN   NaN\n",
      "74       2   5.29         NaN   NaN\n",
      "75       3  18.18         NaN   NaN\n",
      "76       4   5.68         NaN   NaN\n",
      "77       5  20.21         NaN   NaN\n",
      "78       0  24.42         NaN   NaN\n",
      "79       1   5.94         NaN   NaN\n",
      "80       2  15.88         NaN   NaN\n",
      "81       3  16.60         NaN   NaN\n",
      "82       4  25.95         NaN   NaN\n",
      "83       5  20.73         NaN   NaN\n"
     ]
    }
   ],
   "source": [
    "def handle_outliers_and_impute(X_train, X_val, X_test, p_train, p_val, p_test, random_state=0, num_mad=3):\n",
    "    # Number of features\n",
    "    num_features = X_train.shape[2]\n",
    "\n",
    "    # Initialize arrays to store outliers\n",
    "    outliers_train = np.zeros_like(X_train, dtype=bool)\n",
    "    outliers_val = np.zeros_like(X_val, dtype=bool)\n",
    "    outliers_test = np.zeros_like(X_test, dtype=bool)\n",
    "\n",
    "    # Initialize DataFrame to store percentage of outliers\n",
    "    outliers_df = pd.DataFrame(columns=['Feature', 'Train', 'Validation', 'Test'])\n",
    "\n",
    "    # Unique participants\n",
    "    unique_participants = np.unique(np.concatenate([p_train, p_val, p_test]))\n",
    "\n",
    "    for participant in unique_participants:\n",
    "        # Get indices for this participant\n",
    "        train_indices = np.where(p_train == participant)[0]\n",
    "        val_indices = np.where(p_val == participant)[0]\n",
    "        test_indices = np.where(p_test == participant)[0]\n",
    "\n",
    "        for feature in range(num_features):\n",
    "            # Select the feature from each dataset for this participant\n",
    "            X_train_feature = X_train[train_indices, :, feature]\n",
    "            X_val_feature = X_val[val_indices, :, feature]\n",
    "            X_test_feature = X_test[test_indices, :, feature]\n",
    "\n",
    "            # Median Absolute Deviation\n",
    "            mad_train = median_abs_deviation(X_train_feature)\n",
    "            mad_val = median_abs_deviation(X_val_feature)\n",
    "            mad_test = median_abs_deviation(X_test_feature)\n",
    "\n",
    "            threshold_train = num_mad * mad_train\n",
    "            threshold_val = num_mad * mad_val\n",
    "            threshold_test = num_mad * mad_test\n",
    "\n",
    "            outliers_train[train_indices, :, feature] = np.abs(X_train_feature - np.median(X_train_feature)) > threshold_train\n",
    "            outliers_val[val_indices, :, feature] = np.abs(X_val_feature - np.median(X_val_feature)) > threshold_val\n",
    "            outliers_test[test_indices, :, feature] = np.abs(X_test_feature - np.median(X_test_feature)) > threshold_test\n",
    "\n",
    "            # Add percentage of outliers to DataFrame\n",
    "            outliers_df = pd.concat([outliers_df, pd.DataFrame({\n",
    "                'Feature': feature,\n",
    "                'Train': np.mean(outliers_train[train_indices, :, feature]) * 100,\n",
    "                'Validation': np.mean(outliers_val[val_indices, :, feature]) * 100,\n",
    "                'Test': np.mean(outliers_test[test_indices, :, feature]) * 100\n",
    "            }, index=[0])], ignore_index=True)\n",
    "\n",
    "    # Replace outliers with np.nan in the original datasets\n",
    "    X_train = np.where(outliers_train, np.nan, X_train)\n",
    "    X_val = np.where(outliers_val, np.nan, X_val)\n",
    "    X_test = np.where(outliers_test, np.nan, X_test)\n",
    "\n",
    "    # Impute missing values\n",
    "    imputer = IterativeImputer(random_state=random_state) # estimatorestimator object, default=BayesianRidge()\n",
    "\n",
    "    # Reshape the data to 2D, impute, then reshape back to 3D\n",
    "    X_train_shape = X_train.shape\n",
    "    X_val_shape = X_val.shape\n",
    "    X_test_shape = X_test.shape\n",
    "    \n",
    "    X_train = imputer.fit_transform(X_train.reshape(-1, X_train_shape[-1])).reshape(X_train_shape)\n",
    "    X_val = imputer.transform(X_val.reshape(-1, X_val_shape[-1])).reshape(X_val_shape)\n",
    "    X_test = imputer.transform(X_test.reshape(-1, X_test_shape[-1])).reshape(X_test_shape)\n",
    "\n",
    "    print(\"Imputation complete.\")\n",
    "\n",
    "    # Print DataFrame of outliers\n",
    "    print(outliers_df)\n",
    "\n",
    "    return X_train, X_val, X_test\n",
    "\n",
    "X_train_imp_3_pp, X_val_imp_3_pp, X_test_imp_3_pp = handle_outliers_and_impute(X_train, X_val, X_test, p_train, p_val, p_test)\n",
    "X_train_imp_4_pp, X_val_imp_4_pp, X_test_imp_4_pp = handle_outliers_and_impute(X_train, X_val, X_test, p_train, p_val, p_test, num_mad=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        0        1      2         3          4           5\n",
      "Feature              temp      bvp     hr  body_acc  eda_tonic  eda_phasic\n",
      "Original Mean       29.43     0.01  79.35     64.49       0.56        0.00\n",
      "Imputed Mean (3MAD) 29.51     0.56  77.01     64.41       0.49        0.00\n",
      "Imputed Mean (4MAD) 29.52     0.17  77.52     64.41       0.50        0.00\n",
      "Original Std         2.49    78.74  17.15      2.81       0.48        0.04\n",
      "Imputed Std (3MAD)   2.04    27.91  14.64      0.48       0.30        0.01\n",
      "Imputed Std (4MAD)   2.08    33.80  15.21      0.56       0.32        0.01\n",
      "Original Min        21.91 -1578.11  55.30      6.16       0.02       -1.00\n",
      "Imputed Min (3MAD)  25.10  -384.55  49.93     53.44      -0.27       -0.07\n",
      "Imputed Min (4MAD)  25.10  -522.05  55.30     50.05       0.03       -0.09\n",
      "Original Max        34.80  2222.22 149.34    185.16       3.34        0.73\n",
      "Imputed Max (3MAD)  33.25   328.27 149.34     76.48       1.78        0.07\n",
      "Imputed Max (4MAD)  33.66   508.66 149.34     76.48       1.80        0.09\n"
     ]
    }
   ],
   "source": [
    "# %pip install openpyxl\n",
    "num_features = X_train.shape[2]\n",
    "data = []\n",
    "\n",
    "# Assuming 'temp', 'bvp', 'hr', 'body_acc', 'eda_tonic', 'eda_phasic' are feature names\n",
    "feature_names = ['temp', 'bvp', 'hr', 'body_acc', 'eda_tonic', 'eda_phasic']\n",
    "\n",
    "for i in range(num_features):\n",
    "    # Select the feature from each dataset\n",
    "    X_feature = X_train[:, :, i]\n",
    "    X_feature_imp_3 = X_train_imp_3[:, :, i]\n",
    "    X_feature_imp_4 = X_train_imp_4[:, :, i]\n",
    "\n",
    "    data.append({\n",
    "        'Feature': feature_names[i],\n",
    "        'Original Mean': np.nanmean(X_feature),\n",
    "        'Imputed Mean (3MAD)': np.mean(X_feature_imp_3),\n",
    "        'Imputed Mean (4MAD)': np.mean(X_feature_imp_4),\n",
    "        'Original Std': np.nanstd(X_feature),\n",
    "        'Imputed Std (3MAD)': np.std(X_feature_imp_3),\n",
    "        'Imputed Std (4MAD)': np.std(X_feature_imp_4),\n",
    "        'Original Min': np.min(X_feature),\n",
    "        'Imputed Min (3MAD)': np.min(X_feature_imp_3),\n",
    "        'Imputed Min (4MAD)': np.min(X_feature_imp_4),\n",
    "        'Original Max': np.max(X_feature),\n",
    "        'Imputed Max (3MAD)': np.max(X_feature_imp_3),\n",
    "        'Imputed Max (4MAD)': np.max(X_feature_imp_4)\n",
    "    })\n",
    "    \n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "# Make the feature a column instead of an index\n",
    "\n",
    "df = df.transpose()\n",
    "print(df)\n",
    "df= df.to_excel('output/imputation_stats.xlsx', index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Feature       Dataset  Mean   Std      Min     Max\n",
      "0         temp      original 29.43  2.49    21.91   34.80\n",
      "1         temp     imputed_3 29.51  2.04    25.10   33.25\n",
      "2         temp  imputed_3_pp 29.51  2.04    25.10   33.25\n",
      "3         temp     imputed_4 29.52  2.08    25.10   33.66\n",
      "4         temp  imputed_4_pp 29.52  2.08    25.10   33.66\n",
      "5          bvp      original  0.01 78.74 -1578.11 2222.22\n",
      "6          bvp     imputed_3  0.56 27.91  -384.55  328.27\n",
      "7          bvp  imputed_3_pp  0.56 27.91  -384.55  328.27\n",
      "8          bvp     imputed_4  0.17 33.80  -522.05  508.66\n",
      "9          bvp  imputed_4_pp  0.17 33.80  -522.05  508.66\n",
      "10          hr      original 79.35 17.15    55.30  149.34\n",
      "11          hr     imputed_3 77.01 14.64    49.93  149.34\n",
      "12          hr  imputed_3_pp 77.01 14.64    49.93  149.34\n",
      "13          hr     imputed_4 77.52 15.21    55.30  149.34\n",
      "14          hr  imputed_4_pp 77.52 15.21    55.30  149.34\n",
      "15    body_acc      original 64.49  2.81     6.16  185.16\n",
      "16    body_acc     imputed_3 64.41  0.48    53.44   76.48\n",
      "17    body_acc  imputed_3_pp 64.41  0.48    53.44   76.48\n",
      "18    body_acc     imputed_4 64.41  0.56    50.05   76.48\n",
      "19    body_acc  imputed_4_pp 64.41  0.56    50.05   76.48\n",
      "20   eda_tonic      original  0.56  0.48     0.02    3.34\n",
      "21   eda_tonic     imputed_3  0.49  0.30    -0.27    1.78\n",
      "22   eda_tonic  imputed_3_pp  0.49  0.30    -0.27    1.78\n",
      "23   eda_tonic     imputed_4  0.50  0.32     0.03    1.80\n",
      "24   eda_tonic  imputed_4_pp  0.50  0.32     0.03    1.80\n",
      "25  eda_phasic      original  0.00  0.04    -1.00    0.73\n",
      "26  eda_phasic     imputed_3  0.00  0.01    -0.07    0.07\n",
      "27  eda_phasic  imputed_3_pp  0.00  0.01    -0.07    0.07\n",
      "28  eda_phasic     imputed_4  0.00  0.01    -0.09    0.09\n",
      "29  eda_phasic  imputed_4_pp  0.00  0.01    -0.09    0.09\n"
     ]
    }
   ],
   "source": [
    "# %pip install openpyxl\n",
    "num_features = X_train.shape[2]\n",
    "data = []\n",
    "\n",
    "# Assuming 'temp', 'bvp', 'hr', 'body_acc', 'eda_tonic', 'eda_phasic' are feature names\n",
    "feature_names = ['temp', 'bvp', 'hr', 'body_acc', 'eda_tonic', 'eda_phasic']\n",
    "\n",
    "# Adapt loop for new datasets\n",
    "df_names = {\n",
    "    'original' : X_train,\n",
    "    'imputed_3' : X_train_imp_3,\n",
    "    'imputed_3_pp' : X_train_imp_3_pp,\n",
    "    'imputed_4' : X_train_imp_4,\n",
    "    'imputed_4_pp' : X_train_imp_4_pp\n",
    "}\n",
    "\n",
    "for i in range(num_features):\n",
    "    for name, dataset in df_names.items():\n",
    "        # Select the feature from each dataset\n",
    "        X_feature = dataset[:, :, i]\n",
    "\n",
    "        data.append({\n",
    "            'Feature': feature_names[i],\n",
    "            'Dataset': name,\n",
    "            'Mean': np.nanmean(X_feature),\n",
    "            'Std': np.nanstd(X_feature),\n",
    "            'Min': np.nanmin(X_feature),\n",
    "            'Max': np.nanmax(X_feature)\n",
    "        })\n",
    "    \n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "# df.to_excel('output/imputation_stats.xlsx', index=False)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison of normalised data pre and post imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size:  80.23809523809524\n",
      "Val size:  8.333333333333332\n",
      "Test size:  11.428571428571429\n",
      "Size: : (1011, 768, 6)\n",
      "[2.94260982e+01 1.18182592e-02 7.93496987e+01 6.44916532e+01\n",
      " 5.63933428e-01 2.32704741e-05]\n",
      "Initial imputation complete.\n",
      "Missing values before outlier detection:\n",
      "   Train  Validation  Test\n",
      "0   0.00        0.00  0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/koenraijer/Documents/00_Werk/Data_science/Thesis/Analysis/helpers.py:746: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final imputation complete.\n",
      "  Feature  Train  Validation  Test\n",
      "0       0   0.59        0.00  0.00\n",
      "1       1  15.58       14.62  6.23\n",
      "2       2  10.81        0.00  0.00\n",
      "3       3  11.32       15.51 13.71\n",
      "4       4  13.67        0.00  0.00\n",
      "5       5  23.94        9.54 13.29\n",
      "TRAIN STATS\n",
      "   mean_pre  mean_post  std_pre  std_post  min_pre  min_post  max_pre  \\\n",
      "0      0.00       0.00     1.00      1.00    -3.56     -5.13     2.96   \n",
      "1     -0.00       0.00     1.00      1.00   -36.92     -4.59    53.08   \n",
      "2     -0.00      -0.00     1.00      1.00    -2.97     -2.97     5.64   \n",
      "3      0.00      -0.00     1.00      1.00   -19.71     -4.17    66.65   \n",
      "4     -0.00       0.00     1.00      1.00    -6.12     -6.12     8.20   \n",
      "5      0.00      -0.00     1.00      1.00   -23.81     -9.04    25.21   \n",
      "\n",
      "   max_post  \n",
      "0      2.96  \n",
      "1      4.99  \n",
      "2      4.17  \n",
      "3      4.32  \n",
      "4      8.20  \n",
      "5      9.35  \n",
      "VAL STATS\n",
      "   mean_pre  mean_post  std_pre  std_post  min_pre  min_post  max_pre  \\\n",
      "0     -0.00      -0.00     1.00      1.00    -3.37     -3.37     0.65   \n",
      "1      0.00      -0.00     1.00      1.00   -16.73     -2.91    15.60   \n",
      "2     -0.00      -0.00     1.00      1.00    -2.35     -2.35     3.10   \n",
      "3     -0.00      -0.00     1.00      1.00   -13.40     -3.31    25.15   \n",
      "4      0.00       0.00     1.00      1.00    -1.88     -1.88     3.44   \n",
      "5     -0.00       0.00     1.00      1.00   -38.39     -7.13     8.93   \n",
      "\n",
      "   max_post  \n",
      "0      0.65  \n",
      "1      2.78  \n",
      "2      3.10  \n",
      "3      3.47  \n",
      "4      3.44  \n",
      "5      8.99  \n",
      "TEST STATS\n",
      "   mean_pre  mean_post  std_pre  std_post  min_pre  min_post  max_pre  \\\n",
      "0      0.00       0.00     1.00      1.00    -2.52     -2.52     2.06   \n",
      "1     -0.00       0.00     1.00      1.00   -17.40     -4.10    11.97   \n",
      "2     -0.00      -0.00     1.00      1.00    -2.25     -2.25     4.99   \n",
      "3      0.00      -0.00     1.00      1.00   -19.04     -3.24    28.18   \n",
      "4     -0.00       0.00     1.00      1.00    -8.37     -8.37     3.03   \n",
      "5     -0.00       0.00     1.00      1.00   -19.72     -8.14    11.63   \n",
      "\n",
      "   max_post  \n",
      "0      2.06  \n",
      "1      3.95  \n",
      "2      4.99  \n",
      "3      3.59  \n",
      "4      3.03  \n",
      "5      8.46  \n"
     ]
    }
   ],
   "source": [
    "X_train, X_val, X_test, y_train, y_val, y_test, p_train, p_val, p_test = h.prepare_train_val_test_sets(filenames=['output/dl_X_wl24_sr32_original.pkl', 'output/dl_y_wl24_sr32_original.pkl', 'output/dl_p_wl24_sr32_original.pkl'])\n",
    "\n",
    "X_train_pre, X_val_pre, X_test_pre = h.scale_features(X_train, X_val, X_test, p_train, p_val, p_test, normalise=True)\n",
    "X_train, X_val, X_test = h.handle_outliers_and_impute(X_train, X_val, X_test, num_mad=4, verbose=True)\n",
    "X_train_post, X_val_post, X_test_post = h.scale_features(X_train, X_val, X_test, p_train, p_val, p_test, normalise=True)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def create_stats_df(X_pre, X_post):\n",
    "    stats_df = pd.DataFrame({\n",
    "        'mean_pre': np.mean(X_pre, axis=(0, 1)),\n",
    "        'mean_post': np.mean(X_post, axis=(0, 1)),\n",
    "        'std_pre': np.std(X_pre, axis=(0, 1)),\n",
    "        'std_post': np.std(X_post, axis=(0, 1)),\n",
    "        'min_pre': np.min(X_pre, axis=(0, 1)),\n",
    "        'min_post': np.min(X_post, axis=(0, 1)),\n",
    "        'max_pre': np.max(X_pre, axis=(0, 1)),\n",
    "        'max_post': np.max(X_post, axis=(0, 1))\n",
    "    })\n",
    "\n",
    "    return stats_df\n",
    "\n",
    "train_stats_df = create_stats_df(X_train_pre, X_train_post)\n",
    "val_stats_df = create_stats_df(X_val_pre, X_val_post)\n",
    "test_stats_df = create_stats_df(X_test_pre, X_test_post)\n",
    "\n",
    "print(\"TRAIN STATS\")\n",
    "print(train_stats_df)\n",
    "print(\"VAL STATS\")\n",
    "print(val_stats_df)\n",
    "print(\"TEST STATS\")\n",
    "print(test_stats_df)\n",
    "\n",
    "train_stats_df.to_excel('output/train_imputation_stats_prepost_norm.xlsx', index=True)\n",
    "val_stats_df.to_excel('output/val_imputation_stats_prepost_norm.xlsx', index=True)\n",
    "test_stats_df.to_excel('output/test_imputation_stats_prepost_norm.xlsx', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1011, 768, 6)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
