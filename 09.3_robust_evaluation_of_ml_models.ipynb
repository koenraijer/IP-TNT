{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size:  80.23809523809524\n",
      "Val size:  8.333333333333332\n",
      "Test size:  11.428571428571429\n",
      "Size: : (1011, 768, 6)\n"
     ]
    }
   ],
   "source": [
    "# IMPORTS\n",
    "# import sys\n",
    "# sys.path.insert(0, '../Analysis')\n",
    "import helpers as h\n",
    "import empatica_helpers as eh\n",
    "import inquisit_helpers as ih\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from importlib import reload\n",
    "\n",
    "# ML IMPORTS\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Hyperopt\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, space_eval, Trials\n",
    "\n",
    "reload(h), reload(eh), reload(ih)\n",
    "\n",
    "# GLOBAL SETTINGS\n",
    "pd.set_option('display.max_rows', 200)\n",
    "pd.options.display.float_format = '{:.2f}'.format\n",
    "plt.rcParams[\"figure.figsize\"] = (20, 10)\n",
    "plt.style.use('seaborn-v0_8-notebook') # plt.style.use('ggplot'); print(plt.style.available)\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "sr = 32\n",
    "wl = 24 # Window length in seconds\n",
    "\n",
    "# FULL PIPELINE\n",
    "# e_raw, _ = eh.load_empatica(data_folder='input/empatica/', useIBI=False, save=True, plotTrimmings=False, desired_sampling_rate=sr)\n",
    "# i_raw = ih.load_inquisit(data_folder='input/inquisit/', save=True)\n",
    "# ei_raw = h.combine_empatica_and_inquisit(e_raw, i_raw, save=True, sr=sr)\n",
    "# ei_prep = h.clean_and_filter(save=True, normalise=None, sr=sr, window_length=wl)\n",
    "# X, y, p = h.prepare_for_vae(sr=sr, wl=wl, filepath=\"output/ei_prep.csv\", save=True, normalise=False) # Normalisation now happens later in the process. Normalise = False applies the standard scaler to the data.\n",
    "\n",
    "X_train, X_val, X_test, y_train, y_val, y_test, p_train, p_val, p_test = h.prepare_train_val_test_sets(filenames=['output/dl_X_wl24_sr32.pkl', 'output/dl_y_wl24_sr32.pkl', 'output/dl_p_wl24_sr32.pkl'])\n",
    "\n",
    "# Concatenate X_train and X_val to create a new X_train, as well as p_train and p_val to create a new p_train, and y_train and y_val to create a new y_train\n",
    "X_train_raw = np.concatenate((X_train, X_val), axis=0)\n",
    "p_train = np.concatenate((p_train, p_val), axis=0)\n",
    "y_train = np.concatenate((y_train, y_val), axis=0)\n",
    "\n",
    "# BACK TO INPUTS FOR ML MODELS\n",
    "X_test = h.prepare_for_ml(X=X_test, y=y_test, wl=24)\n",
    "\n",
    "# Initialise dicts\n",
    "space_dict = {} # Store the search space for each classifier\n",
    "model_dict = {\n",
    "    'xgb': XGBClassifier,\n",
    "    'glm': LogisticRegression,\n",
    "    'rf': RandomForestClassifier,\n",
    "    'svm': SVC\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: (1116, 36)\n",
      "X_train nans:\n",
      "temp_mean          0\n",
      "temp_std           0\n",
      "temp_min           0\n",
      "temp_max           0\n",
      "temp_skew          0\n",
      "temp_kurt          0\n",
      "bvp_mean           0\n",
      "bvp_std            0\n",
      "bvp_min            0\n",
      "bvp_max            0\n",
      "bvp_skew           0\n",
      "bvp_kurt           0\n",
      "hr_mean            0\n",
      "hr_std             0\n",
      "hr_min             0\n",
      "hr_max             0\n",
      "hr_skew            0\n",
      "hr_kurt            0\n",
      "body_acc_mean      0\n",
      "body_acc_std       0\n",
      "body_acc_min       0\n",
      "body_acc_max       0\n",
      "body_acc_skew      0\n",
      "body_acc_kurt      0\n",
      "eda_tonic_mean     0\n",
      "eda_tonic_std      0\n",
      "eda_tonic_min      0\n",
      "eda_tonic_max      0\n",
      "eda_tonic_skew     0\n",
      "eda_tonic_kurt     0\n",
      "eda_phasic_mean    0\n",
      "eda_phasic_std     0\n",
      "eda_phasic_min     0\n",
      "eda_phasic_max     0\n",
      "eda_phasic_skew    0\n",
      "eda_phasic_kurt    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Print nans in X_train (samples, features)\n",
    "X_train = h.prepare_for_ml(X_train_raw, wl=24, y=y_train)\n",
    "print(f\"X_train: {X_train.shape}\")\n",
    "print(f\"X_train nans:\\n{np.isnan(X_train).sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining hyperparameter spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'xgb': {'window_size': <hyperopt.pyll.base.Apply object at 0x308220590>, 'objective': 'binary:logistic', 'max_depth': <hyperopt.pyll.base.Apply object at 0x3082229d0>, 'min_child_weight': <hyperopt.pyll.base.Apply object at 0x17ec97490>, 'learning_rate': <hyperopt.pyll.base.Apply object at 0x30821da50>, 'subsample': <hyperopt.pyll.base.Apply object at 0x30821ddd0>, 'colsample_bytree': <hyperopt.pyll.base.Apply object at 0x30821e3d0>, 'colsample_bylevel': <hyperopt.pyll.base.Apply object at 0x30821f210>, 'colsample_bynode': <hyperopt.pyll.base.Apply object at 0x30821c550>, 'n_estimators': <hyperopt.pyll.base.Apply object at 0x30ab1c050>, 'gamma': <hyperopt.pyll.base.Apply object at 0x30ab1c990>, 'reg_lambda': <hyperopt.pyll.base.Apply object at 0x30ab1d290>, 'reg_alpha': <hyperopt.pyll.base.Apply object at 0x30ab1db90>, 'scale_pos_weight': 2.024390243902439}, 'glm': {'window_size': <hyperopt.pyll.base.Apply object at 0x30ab1e510>, 'C': <hyperopt.pyll.base.Apply object at 0x30ab1e9d0>, 'penalty': <hyperopt.pyll.base.Apply object at 0x30ab1ef10>, 'solver': <hyperopt.pyll.base.Apply object at 0x30ab1f450>, 'class_weight': 'balanced', 'max_iter': 10000}, 'rf': {'window_size': <hyperopt.pyll.base.Apply object at 0x30ab1fdd0>, 'n_estimators': <hyperopt.pyll.base.Apply object at 0x30ab2d6d0>, 'max_depth': <hyperopt.pyll.base.Apply object at 0x30ab36510>, 'max_features': <hyperopt.pyll.base.Apply object at 0x30ab36b50>, 'min_samples_split': <hyperopt.pyll.base.Apply object at 0x30ab374d0>, 'min_samples_leaf': <hyperopt.pyll.base.Apply object at 0x30ab37bd0>, 'class_weight': 'balanced'}, 'svm': <hyperopt.pyll.base.Apply object at 0x30ab3a850>}\n"
     ]
    }
   ],
   "source": [
    "reload(h)\n",
    "\n",
    "# ------------ XGBoost ------------\n",
    "\n",
    "# Define the hyperparameter space\n",
    "counts = np.unique(y_train, return_counts=True)[1]\n",
    "scale_pos_weight = counts[0] / counts[1] # Recommended by: https://webcache.googleusercontent.com/search?q=cache:https://towardsdatascience.com/a-guide-to-xgboost-hyperparameters-87980c7f44a9&sca_esv=254eb9c569a53dbc&strip=1&vwsrc=0\n",
    "# Default recommendations: https://bradleyboehmke.github.io/xgboost_databricks_tuning/tutorial_docs/xgboost_hyperopt.html\n",
    "\n",
    "xgb_space = {\n",
    "    'window_size': hp.choice('window_size', range(8, 24, 2)),\n",
    "    'objective':'binary:logistic',\n",
    "    'max_depth': hp.choice('max_depth', np.arange(2, 11, dtype=int)),\n",
    "    'min_child_weight': hp.uniform('min_child_weight', 0.1, 15),\n",
    "    'learning_rate': hp.loguniform('learning_rate', np.log(0.0001), np.log(1)),\n",
    "    'subsample': hp.uniform('subsample', 0.5, 1),\n",
    "    'colsample_bytree': hp.uniform('colsample_bytree', 0.5, 1),\n",
    "    'colsample_bylevel': hp.uniform('colsample_bylevel', 0.5, 1),\n",
    "    'colsample_bynode': hp.uniform('colsample_bynode', 0.5, 1),\n",
    "    'n_estimators': hp.choice('n_estimators', range(50, 5000)),\n",
    "    'gamma': hp.choice('gamma', [0, hp.loguniform('gamma_log', np.log(1), np.log(1000))]),\n",
    "    'reg_lambda': hp.choice('reg_lambda', [0, hp.loguniform('reg_lambda_log', np.log(1), np.log(1000))]),\n",
    "    'reg_alpha': hp.choice('reg_alpha', [0, hp.loguniform('reg_alpha_log', np.log(1), np.log(1000))]),\n",
    "    'scale_pos_weight': scale_pos_weight\n",
    "}\n",
    "\n",
    "space_dict['xgb'] = xgb_space\n",
    "\n",
    "# ------------ GLM ------------\n",
    "\n",
    "# Define the hyperparameter space\n",
    "glm_space = {\n",
    "    'window_size': hp.choice('window_size', range(8, 24, 2)),\n",
    "    'C': hp.loguniform('C', np.log(0.001), np.log(1000)),\n",
    "    'penalty': hp.choice('penalty', ['l1', 'l2']),\n",
    "    'solver': hp.choice('solver', ['liblinear', 'saga']), # Only solvers that support both L1 and L2 penalties\n",
    "    'class_weight' : 'balanced',\n",
    "    'max_iter': 10000\n",
    "}\n",
    "\n",
    "space_dict['glm'] = glm_space\n",
    "\n",
    "# ------------ Random Forest ------------\n",
    "\n",
    "# Source: https://towardsdatascience.com/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74\n",
    "rf_space = {\n",
    "    'window_size': hp.choice('window_size', range(8, 24, 2)),\n",
    "    'n_estimators': hp.choice('n_estimators', range(2, 200)),\n",
    "    'max_depth': hp.choice('max_depth', np.arange(2, 101, dtype=int)),\n",
    "    'max_features': hp.choice('max_features', ['log2', 'sqrt', None]),\n",
    "    'min_samples_split': hp.choice('min_samples_split', np.arange(2, 10, dtype=int)),\n",
    "    'min_samples_leaf': hp.choice('min_samples_leaf', np.arange(1, 5, dtype=int)),\n",
    "    'class_weight' : 'balanced'\n",
    "}\n",
    "\n",
    "space_dict['rf'] = rf_space\n",
    "\n",
    "# ------------ SVM ------------\n",
    "\n",
    "# Define the hyperparameter space\n",
    "svm_space = hp.choice('model_type', [\n",
    "    {\n",
    "        'window_size': hp.choice('window_size_linear', range(8, 24, 2)),\n",
    "        'C': hp.loguniform('C_linear', np.log(0.01), np.log(10)),  # Lower range for C\n",
    "        'kernel': 'linear',\n",
    "        'class_weight' : 'balanced'\n",
    "    },\n",
    "    {\n",
    "        'window_size': hp.choice('window_size_rbf', range(8, 24, 2)),\n",
    "        'C': hp.loguniform('C_rbf', np.log(0.01), np.log(10)),\n",
    "        'kernel': 'rbf',\n",
    "        'gamma': hp.loguniform('gamma_rbf', np.log(0.001), np.log(1)),  # Lower range for gamma\n",
    "        'class_weight' : 'balanced'\n",
    "    }\n",
    "])\n",
    "\n",
    "space_dict['svm'] = svm_space\n",
    "\n",
    "print(space_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter optimisation with cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "job exception: name 'vae' is not defined\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?trial/s, best loss=?]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'vae' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 84\u001b[0m\n\u001b[1;32m     81\u001b[0m         best_params_dict[key] \u001b[38;5;241m=\u001b[39m best_params\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m best_params_dict\n\u001b[0;32m---> 84\u001b[0m best_params_dict \u001b[38;5;241m=\u001b[39m \u001b[43moptimise_models\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspace_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_evals\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;66;03m# Save the best parameters\u001b[39;00m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;66;03m# with open('output/ml_best_params_cv_nt.pkl', 'wb') as f:\u001b[39;00m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;66;03m#     pickle.dump(best_params_dict, f)\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[8], line 79\u001b[0m, in \u001b[0;36moptimise_models\u001b[0;34m(space_dict, model_dict, max_evals)\u001b[0m\n\u001b[1;32m     77\u001b[0m best_params_dict \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, space \u001b[38;5;129;01min\u001b[39;00m space_dict\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m---> 79\u001b[0m     best_params \u001b[38;5;241m=\u001b[39m \u001b[43moptimise_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspace\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_evals\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_evals\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest parameters for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_params\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     81\u001b[0m     best_params_dict[key] \u001b[38;5;241m=\u001b[39m best_params\n",
      "Cell \u001b[0;32mIn[8], line 72\u001b[0m, in \u001b[0;36moptimise_model\u001b[0;34m(model, space, max_evals)\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m-\u001b[39mscore, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstatus\u001b[39m\u001b[38;5;124m'\u001b[39m: STATUS_OK, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m'\u001b[39m: params}\n\u001b[1;32m     71\u001b[0m \u001b[38;5;66;03m# Perform the optimisation\u001b[39;00m\n\u001b[0;32m---> 72\u001b[0m best \u001b[38;5;241m=\u001b[39m \u001b[43mfmin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspace\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malgo\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtpe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msuggest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_evals\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_evals\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m best_params \u001b[38;5;241m=\u001b[39m space_eval(space, best)\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m best_params\n",
      "File \u001b[0;32m~/Documents/00_Werk/Data_science/Thesis/Analysis/.conda/lib/python3.11/site-packages/hyperopt/fmin.py:553\u001b[0m, in \u001b[0;36mfmin\u001b[0;34m(fn, space, algo, max_evals, timeout, loss_threshold, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin, points_to_evaluate, max_queue_len, show_progressbar, early_stop_fn, trials_save_file)\u001b[0m\n\u001b[1;32m    550\u001b[0m rval\u001b[38;5;241m.\u001b[39mcatch_eval_exceptions \u001b[38;5;241m=\u001b[39m catch_eval_exceptions\n\u001b[1;32m    552\u001b[0m \u001b[38;5;66;03m# next line is where the fmin is actually executed\u001b[39;00m\n\u001b[0;32m--> 553\u001b[0m \u001b[43mrval\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexhaust\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    555\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_argmin:\n\u001b[1;32m    556\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(trials\u001b[38;5;241m.\u001b[39mtrials) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/Documents/00_Werk/Data_science/Thesis/Analysis/.conda/lib/python3.11/site-packages/hyperopt/fmin.py:356\u001b[0m, in \u001b[0;36mFMinIter.exhaust\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mexhaust\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    355\u001b[0m     n_done \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrials)\n\u001b[0;32m--> 356\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_evals\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mn_done\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mblock_until_done\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masynchronous\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    357\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrials\u001b[38;5;241m.\u001b[39mrefresh()\n\u001b[1;32m    358\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/Documents/00_Werk/Data_science/Thesis/Analysis/.conda/lib/python3.11/site-packages/hyperopt/fmin.py:292\u001b[0m, in \u001b[0;36mFMinIter.run\u001b[0;34m(self, N, block_until_done)\u001b[0m\n\u001b[1;32m    289\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpoll_interval_secs)\n\u001b[1;32m    290\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    291\u001b[0m     \u001b[38;5;66;03m# -- loop over trials and do the jobs directly\u001b[39;00m\n\u001b[0;32m--> 292\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mserial_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrials\u001b[38;5;241m.\u001b[39mrefresh()\n\u001b[1;32m    295\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrials_save_file \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/Documents/00_Werk/Data_science/Thesis/Analysis/.conda/lib/python3.11/site-packages/hyperopt/fmin.py:170\u001b[0m, in \u001b[0;36mFMinIter.serial_evaluate\u001b[0;34m(self, N)\u001b[0m\n\u001b[1;32m    168\u001b[0m ctrl \u001b[38;5;241m=\u001b[39m base\u001b[38;5;241m.\u001b[39mCtrl(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrials, current_trial\u001b[38;5;241m=\u001b[39mtrial)\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 170\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdomain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspec\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctrl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    172\u001b[0m     logger\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjob exception: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mstr\u001b[39m(e))\n",
      "File \u001b[0;32m~/Documents/00_Werk/Data_science/Thesis/Analysis/.conda/lib/python3.11/site-packages/hyperopt/base.py:907\u001b[0m, in \u001b[0;36mDomain.evaluate\u001b[0;34m(self, config, ctrl, attach_attachments)\u001b[0m\n\u001b[1;32m    898\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    899\u001b[0m     \u001b[38;5;66;03m# -- the \"work\" of evaluating `config` can be written\u001b[39;00m\n\u001b[1;32m    900\u001b[0m     \u001b[38;5;66;03m#    either into the pyll part (self.expr)\u001b[39;00m\n\u001b[1;32m    901\u001b[0m     \u001b[38;5;66;03m#    or the normal Python part (self.fn)\u001b[39;00m\n\u001b[1;32m    902\u001b[0m     pyll_rval \u001b[38;5;241m=\u001b[39m pyll\u001b[38;5;241m.\u001b[39mrec_eval(\n\u001b[1;32m    903\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexpr,\n\u001b[1;32m    904\u001b[0m         memo\u001b[38;5;241m=\u001b[39mmemo,\n\u001b[1;32m    905\u001b[0m         print_node_on_error\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrec_eval_print_node_on_error,\n\u001b[1;32m    906\u001b[0m     )\n\u001b[0;32m--> 907\u001b[0m     rval \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpyll_rval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    909\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(rval, (\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mint\u001b[39m, np\u001b[38;5;241m.\u001b[39mnumber)):\n\u001b[1;32m    910\u001b[0m     dict_rval \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mfloat\u001b[39m(rval), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus\u001b[39m\u001b[38;5;124m\"\u001b[39m: STATUS_OK}\n",
      "Cell \u001b[0;32mIn[8], line 62\u001b[0m, in \u001b[0;36moptimise_model.<locals>.objective\u001b[0;34m(params)\u001b[0m\n\u001b[1;32m     60\u001b[0m X_train_fold, X_test_fold \u001b[38;5;241m=\u001b[39m X_train\u001b[38;5;241m.\u001b[39miloc[train_index], X_train\u001b[38;5;241m.\u001b[39miloc[test_index]\n\u001b[1;32m     61\u001b[0m y_train_fold, y_test_fold \u001b[38;5;241m=\u001b[39m y_train[train_index], y_train[test_index]\n\u001b[0;32m---> 62\u001b[0m X_train_fold, y_train_fold \u001b[38;5;241m=\u001b[39m inject_synthetic_data(X_train_fold, y_train_fold, variational_autoencoder\u001b[38;5;241m=\u001b[39m\u001b[43mvae\u001b[49m, window_size\u001b[38;5;241m=\u001b[39mwindow_size, fraction_synthetic\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m, seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m, rebalance\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     63\u001b[0m clf\u001b[38;5;241m.\u001b[39mfit(X_train_fold, y_train_fold)\n\u001b[1;32m     64\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m clf\u001b[38;5;241m.\u001b[39mpredict(X_test_fold)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'vae' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import average_precision_score\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "reload(h)\n",
    "\n",
    "def inject_synthetic_data(X_train_fold, y_train_fold, variational_autoencoder, fraction_synthetic, seed, rebalance):\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # Calculate the number of synthetic samples to generate\n",
    "    num_synthetic_samples = int(len(X_train_fold) * fraction_synthetic)\n",
    "    new_total = len(X_train_fold) + num_synthetic_samples\n",
    "\n",
    "    # If rebalance is True, rebalance the class distribution towards the minority class\n",
    "    if rebalance:\n",
    "        # Calculate the number of samples in each class\n",
    "        num_neg = np.sum(y_train_fold == 0)\n",
    "        num_pos = np.sum(y_train_fold == 1)\n",
    "\n",
    "        add_to_neg = (new_total // 2) - num_neg if ((new_total // 2) - num_neg) > 0 else 0\n",
    "        add_to_pos = (new_total // 2) - num_pos if ((new_total // 2) - num_pos) > 0 else 0\n",
    "\n",
    "        # Generate synthetic samples for each class\n",
    "        synthetic_samples_neg = np.random.normal(size=(add_to_neg, X_train_fold.shape[1]))\n",
    "        synthetic_samples_pos = np.random.normal(size=(add_to_pos, X_train_fold.shape[1]))\n",
    "        # synthetic_samples_neg = variational_autoencoder.generate_samples(add_to_neg, condition=0)\n",
    "        # synthetic_samples_pos = variational_autoencoder.generate_samples(add_to_pos, condition=1)\n",
    "\n",
    "        # Combine the synthetic samples\n",
    "        synthetic_samples = np.concatenate([synthetic_samples_neg, synthetic_samples_pos])\n",
    "        synthetic_labels = np.array([0] * add_to_neg + [1] * add_to_pos)\n",
    "    else:\n",
    "        # Generate synthetic samples for each class\n",
    "        synthetic_samples_neg = vae.generate_samples(num_synthetic_samples // 2, condition=0)\n",
    "        synthetic_samples_pos = vae.generate_samples(num_synthetic_samples // 2, condition=1)\n",
    "\n",
    "        # Combine the synthetic samples\n",
    "        synthetic_samples = np.concatenate([synthetic_samples_neg, synthetic_samples_pos])\n",
    "        synthetic_labels = np.array([0] * len(synthetic_samples_neg) + [1] * len(synthetic_samples_pos))\n",
    "    \n",
    "    # Inject the synthetic samples into the training fold\n",
    "    X_train_fold = np.concatenate([X_train_fold, synthetic_samples])\n",
    "    y_train_fold = np.concatenate([y_train_fold, synthetic_labels])\n",
    "\n",
    "    return X_train_fold, y_train_fold\n",
    "\n",
    "def optimise_model(model, space, max_evals=100):\n",
    "    def objective(params):\n",
    "        # Prepare data\n",
    "        window_size = params.pop('window_size')        \n",
    "        X_train = h.prepare_for_ml(X=X_train_raw, y=y_train, wl=window_size) # p_train, y_train are already defined\n",
    "        \n",
    "        # Create folds\n",
    "        folds = h.create_folds(X_train, y_train, groups=p_train, n_folds=5) # Exhaustive would be 12 folds\n",
    "        # Train and evaluate the model using cross-validation\n",
    "        clf = model(**params, random_state=42)\n",
    "        scores = []\n",
    "\n",
    "        for train_index, test_index in folds:\n",
    "            X_train_fold, X_test_fold = X_train.iloc[train_index], X_train.iloc[test_index]\n",
    "            y_train_fold, y_test_fold = y_train[train_index], y_train[test_index]\n",
    "            X_train_fold, y_train_fold = inject_synthetic_data(X_train_fold, y_train_fold, variational_autoencoder=vae, window_size=window_size, fraction_synthetic=0.5, seed=42, rebalance=True)\n",
    "            clf.fit(X_train_fold, y_train_fold)\n",
    "            y_pred = clf.predict(X_test_fold)\n",
    "            score = average_precision_score(y_test_fold, y_pred)\n",
    "            scores.append(score)\n",
    "        score = np.mean(scores)  # Use the average score\n",
    "\n",
    "        return {'loss': -score, 'status': STATUS_OK, 'params': params}\n",
    "\n",
    "    # Perform the optimisation\n",
    "    best = fmin(objective, space, algo=tpe.suggest, max_evals=max_evals)\n",
    "    best_params = space_eval(space, best)\n",
    "    return best_params\n",
    "\n",
    "def optimise_models(space_dict, model_dict, max_evals=100):\n",
    "    best_params_dict = {}\n",
    "    for key, space in space_dict.items():\n",
    "        best_params = optimise_model(model_dict[key], space, max_evals=max_evals)\n",
    "        print(f\"Best parameters for {key}: {best_params}\")\n",
    "        best_params_dict[key] = best_params\n",
    "    return best_params_dict\n",
    "\n",
    "best_params_dict = optimise_models(space_dict, model_dict, max_evals=100)\n",
    "\n",
    "# Save the best parameters\n",
    "# with open('output/ml_best_params_cv_nt.pkl', 'wb') as f:\n",
    "#     pickle.dump(best_params_dict, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimisation w/ synthetic data injection w/ returning all scores for statistical testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 2/100 [00:08<07:54,  4.84s/trial, best loss: -0.3501198225656577] "
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import average_precision_score\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "reload(h)\n",
    "\n",
    "def optimise_model(model, space, max_evals=100):\n",
    "    def objective(params):\n",
    "        # Prepare data\n",
    "        window_size = params.pop('window_size')        \n",
    "        X_train = h.prepare_for_ml(X=X_train_raw, y=y_train, wl=window_size) # p_train, y_train are already defined\n",
    "        \n",
    "        # Create folds\n",
    "        folds = h.create_folds(X_train, y_train, groups=p_train, n_folds=5) # Exhaustive would be 12 folds\n",
    "        # Train and evaluate the model using cross-validation\n",
    "        clf = model(**params, random_state=42)\n",
    "        scores = []\n",
    "\n",
    "        for train_index, test_index in folds:\n",
    "            X_train_fold, X_test_fold = X_train.iloc[train_index], X_train.iloc[test_index]\n",
    "            y_train_fold, y_test_fold = y_train[train_index], y_train[test_index]\n",
    "            X_train_fold, y_train_fold = h.inject_synthetic_data(X_train_fold, y_train_fold, fraction_synthetic=0.5, seed=42, rebalance=True)\n",
    "            clf.fit(X_train_fold, y_train_fold)\n",
    "            y_pred = clf.predict(X_test_fold)\n",
    "            score = average_precision_score(y_test_fold, y_pred)\n",
    "            scores.append(score)\n",
    "\n",
    "        return {'loss': -np.mean(scores), 'status': STATUS_OK, 'params': params, 'scores': scores}\n",
    "\n",
    "    # Perform the optimisation\n",
    "    trials = Trials()\n",
    "    best = fmin(objective, space, algo=tpe.suggest, max_evals=max_evals, trials=trials)\n",
    "    best_params = space_eval(space, best)\n",
    "    best_scores = trials.best_trial['result']['scores']\n",
    "    return best_params, best_scores\n",
    "\n",
    "def optimise_models(space_dict, model_dict, max_evals=100):\n",
    "    best_params_dict = {}\n",
    "    best_scores_dict = {}\n",
    "    for key, space in space_dict.items():\n",
    "        best_params, best_scores = optimise_model(model_dict[key], space, max_evals=max_evals)\n",
    "        print(f\"Best parameters for {key}: {best_params}\")\n",
    "        print(f\"Best scores for {key}: {best_scores}\")\n",
    "        best_params_dict[key] = best_params\n",
    "        best_scores_dict[key] = best_scores\n",
    "    return best_params_dict, best_scores_dict\n",
    "\n",
    "best_params_dict, best_scores_dict = optimise_models(space_dict, model_dict, max_evals=100)\n",
    "\n",
    "# Save the best parameters\n",
    "# with open('output/ml_best_params_cv_nt.pkl', 'wb') as f:\n",
    "#     pickle.dump(best_params_dict, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
