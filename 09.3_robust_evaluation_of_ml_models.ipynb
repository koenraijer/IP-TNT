{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size:  80.23809523809524\n",
      "Val size:  8.333333333333332\n",
      "Test size:  11.428571428571429\n",
      "Size: : (1011, 768, 6)\n"
     ]
    }
   ],
   "source": [
    "# IMPORTS\n",
    "# import sys\n",
    "# sys.path.insert(0, '../Analysis')\n",
    "import helpers as h\n",
    "import empatica_helpers as eh\n",
    "import inquisit_helpers as ih\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from importlib import reload\n",
    "\n",
    "# ML IMPORTS\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "reload(h), reload(eh), reload(ih)\n",
    "\n",
    "# GLOBAL SETTINGS\n",
    "pd.set_option('display.max_rows', 200)\n",
    "pd.options.display.float_format = '{:.2f}'.format\n",
    "plt.rcParams[\"figure.figsize\"] = (20, 10)\n",
    "plt.style.use('seaborn-v0_8-notebook') # plt.style.use('ggplot'); print(plt.style.available)\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "sr = 32\n",
    "wl = 24 # Window length in seconds\n",
    "\n",
    "# FULL PIPELINE\n",
    "# e_raw, _ = eh.load_empatica(data_folder='input/empatica/', useIBI=False, save=True, plotTrimmings=False, desired_sampling_rate=sr)\n",
    "# i_raw = ih.load_inquisit(data_folder='input/inquisit/', save=True)\n",
    "# ei_raw = h.combine_empatica_and_inquisit(e_raw, i_raw, save=True, sr=sr)\n",
    "# ei_prep = h.clean_and_filter(save=True, normalise=None, sr=sr, window_length=wl)\n",
    "# X, y, p = h.prepare_for_vae(sr=sr, wl=wl, filepath=\"output/ei_prep.csv\", save=True, normalise=False) # Normalisation now happens later in the process. Normalise = False applies the standard scaler to the data.\n",
    "\n",
    "X_train, X_val, X_test, y_train, y_val, y_test, p_train, p_val, p_test = h.prepare_train_val_test_sets(filenames=['output/dl_X_wl24_sr32.pkl', 'output/dl_y_wl24_sr32.pkl', 'output/dl_p_wl24_sr32.pkl'])\n",
    "\n",
    "# Concatenate X_train and X_val to create a new X_train, as well as p_train and p_val to create a new p_train, and y_train and y_val to create a new y_train\n",
    "X_train_raw = np.concatenate((X_train, X_val), axis=0)\n",
    "p_train = np.concatenate((p_train, p_val), axis=0)\n",
    "y_train = np.concatenate((y_train, y_val), axis=0)\n",
    "\n",
    "# BACK TO INPUTS FOR ML MODELS\n",
    "X_test = h.prepare_for_ml(X=X_test, y=y_test, wl=24)\n",
    "\n",
    "# Initialise dicts\n",
    "space_dict = {} # Store the search space for each classifier\n",
    "model_dict = {\n",
    "    'xgb': XGBClassifier,\n",
    "    'glm': LogisticRegression,\n",
    "    'rf': RandomForestClassifier,\n",
    "    'svm': SVC\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: (1116, 36)\n",
      "X_train nans:\n",
      "temp_mean          0\n",
      "temp_std           0\n",
      "temp_min           0\n",
      "temp_max           0\n",
      "temp_skew          0\n",
      "temp_kurt          0\n",
      "bvp_mean           0\n",
      "bvp_std            0\n",
      "bvp_min            0\n",
      "bvp_max            0\n",
      "bvp_skew           0\n",
      "bvp_kurt           0\n",
      "hr_mean            0\n",
      "hr_std             0\n",
      "hr_min             0\n",
      "hr_max             0\n",
      "hr_skew            0\n",
      "hr_kurt            0\n",
      "body_acc_mean      0\n",
      "body_acc_std       0\n",
      "body_acc_min       0\n",
      "body_acc_max       0\n",
      "body_acc_skew      0\n",
      "body_acc_kurt      0\n",
      "eda_tonic_mean     0\n",
      "eda_tonic_std      0\n",
      "eda_tonic_min      0\n",
      "eda_tonic_max      0\n",
      "eda_tonic_skew     0\n",
      "eda_tonic_kurt     0\n",
      "eda_phasic_mean    0\n",
      "eda_phasic_std     0\n",
      "eda_phasic_min     0\n",
      "eda_phasic_max     0\n",
      "eda_phasic_skew    0\n",
      "eda_phasic_kurt    0\n",
      "dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/koenraijer/Documents/00_Werk/Data_science/Thesis/Analysis/helpers.py:514: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  # Add a small amount of noise if the feature is constant\n"
     ]
    }
   ],
   "source": [
    "# Print nans in X_train (samples, features)\n",
    "X_train = h.prepare_for_ml(X_train_raw, wl=24, y=y_train)\n",
    "print(f\"X_train: {X_train.shape}\")\n",
    "print(f\"X_train nans:\\n{np.isnan(X_train).sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining hyperparameter spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'xgb': {'window_size': <hyperopt.pyll.base.Apply object at 0x30c8cb050>, 'objective': 'binary:logistic', 'max_depth': <hyperopt.pyll.base.Apply object at 0x3181321d0>, 'min_child_weight': <hyperopt.pyll.base.Apply object at 0x318132550>, 'learning_rate': <hyperopt.pyll.base.Apply object at 0x318132910>, 'subsample': <hyperopt.pyll.base.Apply object at 0x318132cd0>, 'colsample_bytree': <hyperopt.pyll.base.Apply object at 0x318133090>, 'colsample_bylevel': <hyperopt.pyll.base.Apply object at 0x318133450>, 'colsample_bynode': <hyperopt.pyll.base.Apply object at 0x318133810>, 'n_estimators': <hyperopt.pyll.base.Apply object at 0x3182a8590>, 'gamma': <hyperopt.pyll.base.Apply object at 0x3182a8ed0>, 'reg_lambda': <hyperopt.pyll.base.Apply object at 0x3182a97d0>, 'reg_alpha': <hyperopt.pyll.base.Apply object at 0x3182aa0d0>, 'scale_pos_weight': 2.024390243902439}, 'glm': {'window_size': <hyperopt.pyll.base.Apply object at 0x3182aaa90>, 'C': <hyperopt.pyll.base.Apply object at 0x3182aaf50>, 'penalty': <hyperopt.pyll.base.Apply object at 0x3182ab490>, 'solver': <hyperopt.pyll.base.Apply object at 0x3182ab9d0>, 'class_weight': 'balanced', 'max_iter': 10000}, 'rf': {'window_size': <hyperopt.pyll.base.Apply object at 0x3182b03d0>, 'n_estimators': <hyperopt.pyll.base.Apply object at 0x3182bdc90>, 'max_depth': <hyperopt.pyll.base.Apply object at 0x3182c6ad0>, 'max_features': <hyperopt.pyll.base.Apply object at 0x3182c7110>, 'min_samples_split': <hyperopt.pyll.base.Apply object at 0x3182c7a90>, 'min_samples_leaf': <hyperopt.pyll.base.Apply object at 0x3182c81d0>, 'class_weight': 'balanced'}, 'svm': <hyperopt.pyll.base.Apply object at 0x3182cae50>}\n"
     ]
    }
   ],
   "source": [
    "from hyperopt import fmin, tpe, hp, STATUS_OK, space_eval\n",
    "reload(h)\n",
    "\n",
    "# ------------ XGBoost ------------\n",
    "\n",
    "# Define the hyperparameter space\n",
    "counts = np.unique(y_train, return_counts=True)[1]\n",
    "scale_pos_weight = counts[0] / counts[1] # Recommended by: https://webcache.googleusercontent.com/search?q=cache:https://towardsdatascience.com/a-guide-to-xgboost-hyperparameters-87980c7f44a9&sca_esv=254eb9c569a53dbc&strip=1&vwsrc=0\n",
    "# Default recommendations: https://bradleyboehmke.github.io/xgboost_databricks_tuning/tutorial_docs/xgboost_hyperopt.html\n",
    "\n",
    "xgb_space = {\n",
    "    'window_size': hp.choice('window_size', range(8, 24, 2)),\n",
    "    'objective':'binary:logistic',\n",
    "    'max_depth': hp.choice('max_depth', np.arange(2, 11, dtype=int)),\n",
    "    'min_child_weight': hp.uniform('min_child_weight', 0.1, 15),\n",
    "    'learning_rate': hp.loguniform('learning_rate', np.log(0.0001), np.log(1)),\n",
    "    'subsample': hp.uniform('subsample', 0.5, 1),\n",
    "    'colsample_bytree': hp.uniform('colsample_bytree', 0.5, 1),\n",
    "    'colsample_bylevel': hp.uniform('colsample_bylevel', 0.5, 1),\n",
    "    'colsample_bynode': hp.uniform('colsample_bynode', 0.5, 1),\n",
    "    'n_estimators': hp.choice('n_estimators', range(50, 5000)),\n",
    "    'gamma': hp.choice('gamma', [0, hp.loguniform('gamma_log', np.log(1), np.log(1000))]),\n",
    "    'reg_lambda': hp.choice('reg_lambda', [0, hp.loguniform('reg_lambda_log', np.log(1), np.log(1000))]),\n",
    "    'reg_alpha': hp.choice('reg_alpha', [0, hp.loguniform('reg_alpha_log', np.log(1), np.log(1000))]),\n",
    "    'scale_pos_weight': scale_pos_weight\n",
    "}\n",
    "\n",
    "space_dict['xgb'] = xgb_space\n",
    "\n",
    "# ------------ GLM ------------\n",
    "\n",
    "# Define the hyperparameter space\n",
    "glm_space = {\n",
    "    'window_size': hp.choice('window_size', range(8, 24, 2)),\n",
    "    'C': hp.loguniform('C', np.log(0.001), np.log(1000)),\n",
    "    'penalty': hp.choice('penalty', ['l1', 'l2']),\n",
    "    'solver': hp.choice('solver', ['liblinear', 'saga']), # Only solvers that support both L1 and L2 penalties\n",
    "    'class_weight' : 'balanced',\n",
    "    'max_iter': 10000\n",
    "}\n",
    "\n",
    "space_dict['glm'] = glm_space\n",
    "\n",
    "# ------------ Random Forest ------------\n",
    "\n",
    "# Source: https://towardsdatascience.com/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74\n",
    "rf_space = {\n",
    "    'window_size': hp.choice('window_size', range(8, 24, 2)),\n",
    "    'n_estimators': hp.choice('n_estimators', range(2, 200)),\n",
    "    'max_depth': hp.choice('max_depth', np.arange(2, 101, dtype=int)),\n",
    "    'max_features': hp.choice('max_features', ['log2', 'sqrt', None]),\n",
    "    'min_samples_split': hp.choice('min_samples_split', np.arange(2, 10, dtype=int)),\n",
    "    'min_samples_leaf': hp.choice('min_samples_leaf', np.arange(1, 5, dtype=int)),\n",
    "    'class_weight' : 'balanced'\n",
    "}\n",
    "\n",
    "space_dict['rf'] = rf_space\n",
    "\n",
    "# ------------ SVM ------------\n",
    "\n",
    "# Define the hyperparameter space\n",
    "svm_space = hp.choice('model_type', [\n",
    "    {\n",
    "        'window_size': hp.choice('window_size_linear', range(8, 24, 2)),\n",
    "        'C': hp.loguniform('C_linear', np.log(0.01), np.log(10)),  # Lower range for C\n",
    "        'kernel': 'linear',\n",
    "        'class_weight' : 'balanced'\n",
    "    },\n",
    "    {\n",
    "        'window_size': hp.choice('window_size_rbf', range(8, 24, 2)),\n",
    "        'C': hp.loguniform('C_rbf', np.log(0.01), np.log(10)),\n",
    "        'kernel': 'rbf',\n",
    "        'gamma': hp.loguniform('gamma_rbf', np.log(0.001), np.log(1)),  # Lower range for gamma\n",
    "        'class_weight' : 'balanced'\n",
    "    }\n",
    "])\n",
    "\n",
    "space_dict['svm'] = svm_space\n",
    "\n",
    "print(space_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter optimisation with cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [15:47<00:00,  9.48s/trial, best loss: -0.394812368864122]\n",
      "Best parameters for xgb: {'colsample_bylevel': 0.7171139825644174, 'colsample_bynode': 0.9345302631705191, 'colsample_bytree': 0.9176973301395794, 'gamma': 0, 'learning_rate': 0.09697428904596696, 'max_depth': 5, 'min_child_weight': 4.739771838795894, 'n_estimators': 1086, 'objective': 'binary:logistic', 'reg_alpha': 0, 'reg_lambda': 984.598209912321, 'scale_pos_weight': 2.024390243902439, 'subsample': 0.9011931425417188, 'window_size': 20}\n",
      "100%|██████████| 100/100 [03:11<00:00,  1.92s/trial, best loss: -0.3834990380677798]\n",
      "Best parameters for glm: {'C': 2.9440879704073484, 'class_weight': 'balanced', 'max_iter': 10000, 'penalty': 'l2', 'solver': 'liblinear', 'window_size': 18}\n",
      "100%|██████████| 100/100 [03:08<00:00,  1.88s/trial, best loss: -0.3696295979324021]\n",
      "Best parameters for rf: {'class_weight': 'balanced', 'max_depth': 28, 'max_features': 'sqrt', 'min_samples_leaf': 2, 'min_samples_split': 6, 'n_estimators': 11, 'window_size': 20}\n",
      "100%|██████████| 100/100 [07:34<00:00,  4.55s/trial, best loss: -0.3824617804873391]\n",
      "Best parameters for svm: {'C': 7.173586682123113, 'class_weight': 'balanced', 'kernel': 'linear', 'window_size': 18}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import average_precision_score\n",
    "from hyperopt import fmin, tpe, STATUS_OK, space_eval\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "reload(h)\n",
    "\n",
    "def inject_synthetic_data(X_train_fold, y_train_fold, variational_autoencoder, window_size, fraction_synthetic, seed, rebalance):\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # Calculate the number of synthetic samples to generate\n",
    "    num_synthetic_samples = int(len(X_train_fold) * fraction_synthetic)\n",
    "    new_total = len(X_train_fold) + num_synthetic_samples\n",
    "\n",
    "    # If rebalance is True, rebalance the class distribution towards the minority class\n",
    "    if rebalance:\n",
    "        # Calculate the number of samples in each class\n",
    "        num_neg = np.sum(y_train_fold == 0)\n",
    "        num_pos = np.sum(y_train_fold == 1)\n",
    "\n",
    "        add_to_neg = (new_total // 2) - num_neg if ((new_total // 2) - num_neg) > 0 else 0\n",
    "        add_to_pos = (new_total // 2) - num_pos if ((new_total // 2) - num_pos) > 0 else 0\n",
    "\n",
    "        # Generate synthetic samples for each class\n",
    "        synthetic_samples_neg = vae.generate_samples(add_to_neg, condition=0)\n",
    "        synthetic_samples_pos = vae.generate_samples(add_to_pos, condition=1)\n",
    "\n",
    "        # Combine the synthetic samples\n",
    "        synthetic_samples = np.concatenate([synthetic_samples_neg, synthetic_samples_pos])\n",
    "        synthetic_labels = np.array([0] * add_to_neg + [1] * add_to_pos)\n",
    "    else:\n",
    "        # Generate synthetic samples for each class\n",
    "        synthetic_samples_neg = vae.generate_samples(num_synthetic_samples // 2, condition=0)\n",
    "        synthetic_samples_pos = vae.generate_samples(num_synthetic_samples // 2, condition=1)\n",
    "\n",
    "        # Combine the synthetic samples\n",
    "        synthetic_samples = np.concatenate([synthetic_samples_neg, synthetic_samples_pos])\n",
    "        synthetic_labels = np.array([0] * len(synthetic_samples_neg) + [1] * len(synthetic_samples_pos))\n",
    "    \n",
    "    # Inject the synthetic samples into the training fold\n",
    "    X_train_fold = np.concatenate([X_train_fold, synthetic_samples])\n",
    "    y_train_fold = np.concatenate([y_train_fold, synthetic_labels])\n",
    "\n",
    "    return X_train_fold, y_train_fold\n",
    "\n",
    "def optimise_model(model, space, max_evals=100):\n",
    "    def objective(params):\n",
    "        # Prepare data\n",
    "        window_size = params.pop('window_size')        \n",
    "        X_train = h.prepare_for_ml(X=X_train_raw, y=y_train, wl=window_size) # p_train, y_train are already defined\n",
    "        \n",
    "        # Create folds\n",
    "        folds = h.create_folds(X_train, y_train, groups=p_train, n_folds=5) # Exhaustive would be 12 folds\n",
    "        # Train and evaluate the model using cross-validation\n",
    "        clf = model(**params, random_state=42)\n",
    "        scores = []\n",
    "\n",
    "        for train_index, test_index in folds:\n",
    "            X_train_fold, X_test_fold = X_train.iloc[train_index], X_train.iloc[test_index]\n",
    "            y_train_fold, y_test_fold = y_train[train_index], y_train[test_index]\n",
    "            # X_train_fold, y_train_fold = inject_synthetic_data(X_train_fold, y_train_fold, variational_autoencoder=vae, window_size=window_size, fraction_synthetic=0.5, seed=42, rebalance=True)\n",
    "            clf.fit(X_train_fold, y_train_fold)\n",
    "            y_pred = clf.predict(X_test_fold)\n",
    "            score = average_precision_score(y_test_fold, y_pred)\n",
    "            scores.append(score)\n",
    "        score = np.mean(scores)  # Use the average score\n",
    "\n",
    "        return {'loss': -score, 'status': STATUS_OK, 'params': params}\n",
    "\n",
    "    # Perform the optimisation\n",
    "    best = fmin(objective, space, algo=tpe.suggest, max_evals=max_evals)\n",
    "    best_params = space_eval(space, best)\n",
    "    return best_params\n",
    "\n",
    "def optimise_models(space_dict, model_dict, max_evals=100):\n",
    "    best_params_dict = {}\n",
    "    for key, space in space_dict.items():\n",
    "        best_params = optimise_model(model_dict[key], space, max_evals=max_evals)\n",
    "        print(f\"Best parameters for {key}: {best_params}\")\n",
    "        best_params_dict[key] = best_params\n",
    "    return best_params_dict\n",
    "\n",
    "best_params_dict = optimise_models(space_dict, model_dict, max_evals=100)\n",
    "\n",
    "# Save the best parameters\n",
    "# with open('output/ml_best_params_cv_nt.pkl', 'wb') as f:\n",
    "#     pickle.dump(best_params_dict, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
